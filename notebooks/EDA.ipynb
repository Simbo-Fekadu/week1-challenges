{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15500410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: c:\\Users\\Simbo\\Desktop\\week1-challenge\\notebooks\n",
      "Data loaded successfully. Shape: (1407328, 6)\n",
      "\n",
      "Missing values before cleaning:\n",
      "Unnamed: 0    0\n",
      "headline      0\n",
      "url           0\n",
      "publisher     0\n",
      "date          0\n",
      "stock         0\n",
      "dtype: int64\n",
      "\n",
      "Missing dates after parsing: 0\n",
      "\n",
      "Headline Length Statistics:\n",
      "count    1.407328e+06\n",
      "mean     7.312051e+01\n",
      "std      4.073531e+01\n",
      "min      3.000000e+00\n",
      "25%      4.700000e+01\n",
      "50%      6.400000e+01\n",
      "75%      8.700000e+01\n",
      "max      5.120000e+02\n",
      "Name: headline_length, dtype: float64\n",
      "\n",
      "Articles per Publisher:\n",
      "publisher\n",
      "Paul Quintaro        228373\n",
      "Lisa Levin           186979\n",
      "Benzinga Newsdesk    150484\n",
      "Charles Gross         96732\n",
      "Monica Gerson         82380\n",
      "Eddie Staley          57254\n",
      "Hal Lindon            49047\n",
      "ETF Professor         28489\n",
      "Juan Lopez            28438\n",
      "Benzinga Staff        28114\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 Common Words in Headlines:\n",
      "[('stocks', 161776), ('from', 120805), ('market', 120558), ('shares', 114313), ('reports', 108710), ('update', 91723), ('earnings', 87399), ('with', 84257), ('sales', 79645), ('benzinga', 74516), ('week', 69572), ('announces', 66591), ('price', 64407), ('downgrades', 61959), ('trading', 61182), ('raises', 57802), ('upgrades', 56811), ('target', 54714), ('maintains', 52961), ('down', 50060)]\n",
      "\n",
      "Sentiment Statistics:\n",
      "count    1.407328e+06\n",
      "mean     4.905657e-02\n",
      "std      1.830652e-01\n",
      "min     -1.000000e+00\n",
      "25%      0.000000e+00\n",
      "50%      0.000000e+00\n",
      "75%      0.000000e+00\n",
      "max      1.000000e+00\n",
      "Name: sentiment, dtype: float64\n",
      "\n",
      "Top 10 Publisher Domains:\n",
      "domain\n",
      "Paul Quintaro        228373\n",
      "Lisa Levin           186979\n",
      "Benzinga Newsdesk    150484\n",
      "Charles Gross         96732\n",
      "Monica Gerson         82380\n",
      "Eddie Staley          57254\n",
      "Hal Lindon            49047\n",
      "ETF Professor         28489\n",
      "Juan Lopez            28438\n",
      "Benzinga Staff        28114\n",
      "Name: count, dtype: int64\n",
      "EDA completed. Visualizations saved in reports/. Processed data saved in data/.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Print current working directory for debugging\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    data = pd.read_csv('../data/raw_analyst_ratings/raw_analyst_ratings.csv')\n",
    "    print(\"Data loaded successfully. Shape:\", data.shape)\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"CSV file not found. Check the path: '../data/raw_analyst_ratings/raw_analyst_ratings.csv'\")\n",
    "\n",
    "# --- 1. Data Cleaning ---\n",
    "# Handle missing values\n",
    "print(\"\\nMissing values before cleaning:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "data = data.dropna(subset=['headline', 'publisher', 'date'])  # Drop rows with critical missing data\n",
    "\n",
    "# --- 2. Date Parsing Fix ---\n",
    "def parse_dates(date_str):\n",
    "    try:\n",
    "        # Try parsing with timezone first\n",
    "        return pd.to_datetime(date_str, format='%Y-%m-%d %H:%M:%S%z', utc=True)\n",
    "    except ValueError:\n",
    "        try:\n",
    "            # Fallback to naive datetime\n",
    "            return pd.to_datetime(date_str, format='%Y-%m-%d %H:%M:%S', utc=True)\n",
    "        except ValueError:\n",
    "            return pd.NaT\n",
    "\n",
    "data['date'] = data['date'].apply(parse_dates)\n",
    "data = data.dropna(subset=['date'])  # Remove rows where date parsing failed\n",
    "print(\"\\nMissing dates after parsing:\", data['date'].isna().sum())\n",
    "\n",
    "# --- 3. Descriptive Statistics ---\n",
    "# Headline length\n",
    "data['headline_length'] = data['headline'].apply(len)\n",
    "print(\"\\nHeadline Length Statistics:\")\n",
    "print(data['headline_length'].describe())\n",
    "\n",
    "# Articles per publisher\n",
    "publisher_counts = data['publisher'].value_counts()\n",
    "print(\"\\nArticles per Publisher:\")\n",
    "print(publisher_counts.head(10))  # Top 10 publishers\n",
    "\n",
    "# --- 4. Time Series Analysis ---\n",
    "# Publication frequency\n",
    "data['date_only'] = data['date'].dt.date\n",
    "date_counts = data['date_only'].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "date_counts.plot(kind='line', title='Article Publication Frequency Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.savefig('reports/publication_trend.png')\n",
    "plt.close()\n",
    "\n",
    "# Hourly distribution\n",
    "data['hour'] = data['date'].dt.hour\n",
    "hourly_counts = data['hour'].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "hourly_counts.plot(kind='bar', title='Article Publication by Hour')\n",
    "plt.xlabel('Hour of Day (UTC)')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.savefig('reports/hourly_publication.png')\n",
    "plt.close()\n",
    "\n",
    "# --- 5. Text Analysis (Topic Modeling) ---\n",
    "def extract_keywords(text):\n",
    "    words = re.findall(r'\\w+', text.lower())\n",
    "    return [word for word in words if len(word) > 3]  # Filter short words\n",
    "\n",
    "all_words = data['headline'].apply(extract_keywords).explode()\n",
    "common_words = Counter(all_words).most_common(20)\n",
    "print(\"\\nTop 20 Common Words in Headlines:\")\n",
    "print(common_words)\n",
    "\n",
    "# Basic sentiment analysis\n",
    "data['sentiment'] = data['headline'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "print(\"\\nSentiment Statistics:\")\n",
    "print(data['sentiment'].describe())\n",
    "\n",
    "# --- 6. Publisher Analysis ---\n",
    "# Extract domains if publishers are email addresses\n",
    "data['domain'] = data['publisher'].apply(lambda x: x.split('@')[-1] if '@' in str(x) else x)\n",
    "domain_counts = data['domain'].value_counts()\n",
    "print(\"\\nTop 10 Publisher Domains:\")\n",
    "print(domain_counts.head(10))\n",
    "\n",
    "# --- 7. Save Processed Data ---\n",
    "data.to_csv('../data/processed_analyst_ratings.csv', index=False)\n",
    "\n",
    "# Commit the results\n",
    "print(\"EDA completed. Visualizations saved in reports/. Processed data saved in data/.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
